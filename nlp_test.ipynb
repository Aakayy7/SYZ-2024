{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification, BertForSequenceClassification , BertTokenizerFast\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the NER model and tokenizer\n",
    "ner_model = BertForTokenClassification.from_pretrained(\"bert_ner\")\n",
    "ner_tokenizer = BertTokenizerFast.from_pretrained(\"bert_ner\")\n",
    "\n",
    "# Load the text classification model and tokenizer\n",
    "clf_model = BertForSequenceClassification.from_pretrained(\"bert_clf\")\n",
    "clf_tokenizer = BertTokenizer.from_pretrained(\"bert_clf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0: \"O\",             # Outside any entity\n",
    "    1: \"ANAT\",          # Anatomical entity\n",
    "    2: \"OBS-ABSENT\",    # Observation absent\n",
    "    3: \"OBS-PRESENT\",   # Observation present\n",
    "    4: \"OBS-UNCERTAIN\", # Observation uncertain\n",
    "    5: \"PAD\"            # Padding token\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1).item()\n",
    "    predictions = predictions + 1\n",
    "    return str(predictions)  # Convert prediction to string\n",
    "\n",
    "\n",
    "def predict_entities(text, model, tokenizer, label_map, merge_threshold=4):\n",
    "    \"\"\"\n",
    "    Predict entities and merge contiguous entities with the same label if they are within a certain threshold.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to process.\n",
    "        model: The NER model.\n",
    "        tokenizer: The tokenizer for the NER model.\n",
    "        label_map (dict): A mapping from label IDs to label names.\n",
    "        merge_threshold (int): The maximum gap between entities to consider them as the same entity.\n",
    "    \n",
    "    Returns:\n",
    "        List[List]: A list of merged entities.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, return_offsets_mapping=True)\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")[0].tolist()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "    \n",
    "    entities = []\n",
    "    current_start = None\n",
    "    current_label = None\n",
    "    for idx, pred in enumerate(predictions):\n",
    "        label = label_map.get(pred, \"O\")\n",
    "        start, end = offset_mapping[idx]\n",
    "        \n",
    "        if label == 'O' or label == 'PAD':\n",
    "            if current_label and current_label != 'O':\n",
    "                # End of an entity\n",
    "                entities.append([current_start, offset_mapping[idx-1][1], current_label])\n",
    "                current_start = None\n",
    "                current_label = None\n",
    "        else:\n",
    "            if label == current_label and current_start is not None:\n",
    "                # Extend the current entity\n",
    "                end = offset_mapping[idx][1]\n",
    "            else:\n",
    "                # Start a new entity\n",
    "                if current_label and current_label != 'O' and current_start is not None:\n",
    "                    entities.append([current_start, offset_mapping[idx-1][1], current_label])\n",
    "                current_start = start\n",
    "                current_label = label\n",
    "    \n",
    "    # Add the last entity if applicable\n",
    "    if current_label and current_label != 'O' and current_start is not None:\n",
    "        entities.append([current_start, offset_mapping[-1][1], current_label])\n",
    "    \n",
    "    # Merge entities that are close to each other\n",
    "    merged_entities = []\n",
    "    if entities:\n",
    "        merged_start, merged_end, merged_label = entities[0]\n",
    "        \n",
    "        for start, end, label in entities[1:]:\n",
    "            if label == merged_label and start <= merged_end + merge_threshold:\n",
    "                # Extend the current merged entity\n",
    "                merged_end = max(merged_end, end)\n",
    "            else:\n",
    "                # Add the current merged entity to the list\n",
    "                merged_entities.append([merged_start, merged_end, merged_label])\n",
    "                merged_start, merged_end, merged_label = start, end, label\n",
    "        \n",
    "        # Add the last merged entity\n",
    "        merged_entities.append([merged_start, merged_end, merged_label])\n",
    "    \n",
    "    return merged_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_entities(text, model, tokenizer, label_map, merge_threshold=4):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, return_offsets_mapping=True)\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")[0].tolist()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "    \n",
    "    entities = []\n",
    "    current_start = None\n",
    "    current_label = None\n",
    "    for idx, pred in enumerate(predictions):\n",
    "        label = label_map.get(pred, \"O\")\n",
    "        start, end = offset_mapping[idx]\n",
    "        \n",
    "        if label == 'O' or label == 'PAD':\n",
    "            if current_label and current_label != 'O':\n",
    "                # End of an entity\n",
    "                entities.append([current_start, offset_mapping[idx-1][1], current_label])\n",
    "                current_start = None\n",
    "                current_label = None\n",
    "        else:\n",
    "            if label == current_label and current_start is not None:\n",
    "                # Extend the current entity\n",
    "                end = offset_mapping[idx][1]\n",
    "            else:\n",
    "                # Start a new entity\n",
    "                if current_label and current_label != 'O' and current_start is not None:\n",
    "                    entities.append([current_start, offset_mapping[idx-1][1], current_label])\n",
    "                current_start = start\n",
    "                current_label = label\n",
    "    \n",
    "    # Add the last entity if applicable\n",
    "    if current_label and current_label != 'O' and current_start is not None:\n",
    "        entities.append([current_start, offset_mapping[-1][1], current_label])\n",
    "    \n",
    "    # Merge entities that are close to each other\n",
    "    merged_entities = []\n",
    "    if entities:\n",
    "        merged_start, merged_end, merged_label = entities[0]\n",
    "        \n",
    "        for start, end, label in entities[1:]:\n",
    "            if label == merged_label and start <= merged_end + merge_threshold:\n",
    "                # Extend the current merged entity\n",
    "                merged_end = max(merged_end, end)\n",
    "            else:\n",
    "                # Add the current merged entity to the list\n",
    "                merged_entities.append([merged_start, merged_end, merged_label])\n",
    "                merged_start, merged_end, merged_label = start, end, label\n",
    "        \n",
    "        # Add the last merged entity\n",
    "        merged_entities.append([merged_start, merged_end, merged_label])\n",
    "    \n",
    "    return merged_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_data(data, clf_model, clf_tokenizer, ner_model, ner_tokenizer, label_map):\n",
    "    for item in data['tahminler']:\n",
    "        text = item['text']\n",
    "\n",
    "        # Predict the class using the classification model\n",
    "        predicted_class = classify_text(text, clf_model, clf_tokenizer)\n",
    "        item['cats'].append(predicted_class)\n",
    "\n",
    "        # Predict the entities using the NER model\n",
    "        predicted_entities = predict_entities(text, ner_model, ner_tokenizer, label_map)\n",
    "\n",
    "        # Format the predicted entities as [start, end, label]\n",
    "        formatted_entities = [\n",
    "            [entity[0], entity[1], entity[2]] for entity in predicted_entities\n",
    "        ]\n",
    "\n",
    "        # Replace the entities with the formatted entities\n",
    "        item['entities'] = formatted_entities\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_data(data, clf_model, clf_tokenizer, ner_model, ner_tokenizer, label_map):\n",
    "    for item in data['tahminler']:\n",
    "        text = item['text']\n",
    "\n",
    "        # Predict the class using the classification model\n",
    "        predicted_class = classify_text(text, clf_model, clf_tokenizer)\n",
    "        item['cats'].append(predicted_class)\n",
    "\n",
    "        # Predict the entities using the NER model\n",
    "        predicted_entities = predict_entities(text, ner_model, ner_tokenizer, label_map)\n",
    "\n",
    "        # Format the predicted entities\n",
    "        formatted_entities = [\n",
    "            [entity[0], entity[1], entity[2]] for entity in predicted_entities\n",
    "        ]\n",
    "\n",
    "        # Extend the 'entities' list with formatted entities\n",
    "        item['entities'].extend(formatted_entities)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data has been saved to 'processed_data.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(\"nlp_test_dataset.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)  # Load the JSON content into a Python dictionary\n",
    "\n",
    "# Process the data using both models\n",
    "processed_data = process_json_data(data, clf_model, clf_tokenizer, ner_model, ner_tokenizer, label_map)\n",
    "\n",
    "# Save the processed data to a new JSON file\n",
    "with open(\"293962_LayerLords_2.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    json.dump(processed_data, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Processed data has been saved to 'processed_data.json'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
