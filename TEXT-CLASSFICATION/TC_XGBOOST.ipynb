{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10coLkug5yQN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix , accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.utils import resample\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing, decomposition\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "%matplotlib inline\n",
        "from yellowbrick.text import FreqDistVisualizer\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset\n",
        "from transformers import BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('main_dataframe.csv')"
      ],
      "metadata": {
        "id": "h7fGiMXS8Ej8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.rename(columns={'Content': 'content', 'BIRADS Score': 'label'}, inplace=True)"
      ],
      "metadata": {
        "id": "L4Hbdn9u8E3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic preprocessing\n",
        "data['content'] = data['content'].str.lower().str.replace(r'[^\\w\\s]', '')\n"
      ],
      "metadata": {
        "id": "DmN5xjJc8GrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "turkish_stop_words = [\n",
        "    'acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'biri', 'birkaç', 'birşey', 'biz', 'bu', 'çok', 'çünkü',\n",
        "    'da', 'daha', 'de', 'defa', 'diye', 'eğer', 'en', 'gibi', 'hem', 'hep', 'hepsi', 'her', 'hiç', 'için', 'ile',\n",
        "    'ise', 'kez', 'ki', 'kim', 'mı', 'mu', 'mü', 'nasıl', 'ne', 'neden', 'nerde', 'nerede', 'nereye', 'niçin',\n",
        "    'niye', 'o', 'sanki', 'şey', 'siz', 'şu', 'tüm', 've', 'veya', 'ya', 'yani'\n",
        "]\n",
        "\n",
        "\n",
        "# Split the data\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    data['content'], data['label'], test_size=0.125, stratify=data['label'], random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
        "val_labels_encoded = label_encoder.transform(val_labels)\n",
        "\n",
        "\n",
        "\n",
        "# Initialize and fit-transform the TF-IDF vectorizer\n",
        "tfidf = TfidfVectorizer(strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
        "                        ngram_range=(1, 3), use_idf=True, smooth_idf=True, sublinear_tf=True,\n",
        "                        stop_words=turkish_stop_words)\n",
        "\n",
        "tfidf.fit(list(train_texts) + list(val_texts))\n",
        "X_train_tfidf = tfidf.transform(train_texts)\n",
        "X_val_tfidf = tfidf.transform(val_texts)\n",
        "\n",
        "# Convert the TF-IDF matrices to DMatrix, the data structure that XGBoost uses\n",
        "dtrain = xgb.DMatrix(X_train_tfidf, label=train_labels_encoded)\n",
        "dval = xgb.DMatrix(X_val_tfidf, label=val_labels_encoded)\n",
        "\n",
        "\n",
        "# Set the parameters for XGBoost\n",
        "params = {\n",
        "    'objective': 'multi:softmax',  # Multi-class classification\n",
        "    'num_class': 5,                # Number of classes\n",
        "    'max_depth': 6,                # Maximum depth of the tree\n",
        "    'eta': 0.3,                    # Learning rate\n",
        "    'eval_metric': 'mlogloss',     # Evaluation metric\n",
        "    'seed': 42                     # Random seed for reproducibility\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "num_rounds = 100  # Number of boosting rounds\n",
        "bst = xgb.train(params, dtrain, num_rounds)\n",
        "\n",
        "# Make predictions\n",
        "val_predictions = bst.predict(dval)\n",
        "\n",
        "target_names = [\"BIRADS-1\",\"BIRADS-2\",\"BIRADS-3\",\"BIRADS-4\",\"BIRADS-5\"]\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(val_labels_encoded, val_predictions, target_names= target_names))\n",
        "\n",
        "# Print confusion matrix\n",
        "conf_matrix = confusion_matrix(val_labels_encoded, val_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1JZtoms79Qq",
        "outputId": "78f86e4e-6fd9-4cae-c5a3-c4d507bda87c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['birkac', 'birsey', 'cok', 'cunku', 'eger', 'hic', 'icin', 'nicin', 'sey', 'su', 'tum'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    BIRADS-1       0.67      0.33      0.44         6\n",
            "    BIRADS-2       0.89      0.84      0.87        50\n",
            "    BIRADS-3       0.92      1.00      0.96        56\n",
            "    BIRADS-4       0.97      0.99      0.98        69\n",
            "    BIRADS-5       1.00      1.00      1.00        69\n",
            "\n",
            "    accuracy                           0.95       250\n",
            "   macro avg       0.89      0.83      0.85       250\n",
            "weighted avg       0.94      0.95      0.94       250\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 2  4  0  0  0]\n",
            " [ 1 42  5  2  0]\n",
            " [ 0  0 56  0  0]\n",
            " [ 0  1  0 68  0]\n",
            " [ 0  0  0  0 69]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "import pickle\n",
        "\n",
        "# Assuming `bst` is your trained XGBoost model\n",
        "\n",
        "# Define the file path where you want to save the model\n",
        "model_file_path = 'xgboost_model.bin'\n",
        "\n",
        "# Save the model\n",
        "bst.save_model(model_file_path)\n",
        "\n",
        "# If you also want to save the label encoder for later use\n",
        "encoder_file_path = 'label_encoder.pkl'\n",
        "with open(encoder_file_path, 'wb') as encoder_file:\n",
        "    pickle.dump(label_encoder, encoder_file)\n"
      ],
      "metadata": {
        "id": "fa_D9_4TRn56"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}